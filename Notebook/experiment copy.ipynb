{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key=os.getenv(\"Google_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api key found\n"
     ]
    }
   ],
   "source": [
    "if google_api_key == \"\":\n",
    "    print(\"API Key not found..!\")\n",
    "else:\n",
    "    print(\"Api key found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc\\qabot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from IPython.display import Markdown, display\n",
    "import google.generativeai as genai \n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    if 'generateContent' in models.supported_generation_methods:\n",
    "        print(models.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byθthat generates a current token based on a context of the previous i−1tokensy1:i−1, the original\n",
      "inputxand a retrieved passage z.\n",
      "To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\n",
      "We propose two models that marginalize over the latent documents in different ways to produce a\n",
      "distribution over generated text. In one approach, RAG-Sequence , the model uses the same document\n",
      "to predict each target token. The second approach, RAG-Token , can predict each target token based\n",
      "on a different document. In the following, we formally introduce both models and then describe the\n",
      "pηandpθcomponents, as well as the training and decoding procedure.\n",
      "2.1 Models\n",
      "RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\n",
      "the complete sequence . Technically, it treats the retrieved document as a single latent variable that\n",
      "is marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the\n",
      "top K documents are retrieved using the retriever, and the generator produces the output sequence\n",
      "probability for each document, which are then marginalized,\n",
      "pRAG-Sequence (y|x)≈∑\n",
      "z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑\n",
      "z∈top-k(p(·|x))pη(z|x)N∏\n",
      "ipθ(yi|x,z,y 1:i−1)\n",
      "RAG-Token Model In the RAG-Token model we can draw a different latent document for each\n",
      "target token and marginalize accordingly. This allows the generator to choose content from several\n",
      "documents when producing an answer. Concretely, the top K documents are retrieved using the\n",
      "retriever, and then the generator produces a distribution for the next output token for each document,\n",
      "before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\n",
      "pRAG-Token (y|x)≈N∏\n",
      "i∑\n",
      "z∈top-k(p(·|x))pη(z|x)pθ(yi|x,z,y 1:i−1)\n",
      "Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\n",
      "as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n",
      "2.2 Retriever: DPR\n",
      "The retrieval component pη(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture:\n",
      "pη(z|x)∝exp(\n",
      "d(z)⊤q(x))\n",
      "d(z) =BERTd(z),q(x) =BERTq(x)\n",
      "where d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],\n",
      "andq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating\n",
      "top-k (pη(·|x)), the list ofkdocumentszwith highest prior probability pη(z|x), is a Maximum Inner\n",
      "Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use\n",
      "a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\n",
      "retriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and\n",
      "Natural Questions [29]. We refer to the document index as the non-parametric memory .\n",
      "2.3 Generator: BART\n",
      "The generator component pθ(yi|x,z,y 1:i−1)could be modelled using any encoder-decoder. We use\n",
      "BART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input\n",
      "xwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\n",
      "pre-trained using a denoising objective and a variety of different noising functions. It has obtained\n",
      "state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\n",
      "models [32]. We refer to the BART generator parameters θas the parametric memory henceforth.\n",
      "2.4 Training\n",
      "We jointly train the retriever and generator components without any direct supervision on what\n",
      "document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"../Data\")\n",
    "doc = documents.load_data()\n",
    "print(doc[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemini(models=\"gemini-pro\", api_key=google_api_key)\n",
    "gemni_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sc\\AppData\\Local\\Temp\\ipykernel_6428\\905837816.py:2: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=model, embed_model=gemni_embed_model, chunk_size=800, chunk_overlap=20)\n"
     ]
    }
   ],
   "source": [
    "# Configure Service Context\n",
    "service_context = ServiceContext.from_defaults(llm=model, embed_model=gemni_embed_model, chunk_size=800, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(doc, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we can jointly train the retriever and generator components without any direct supervision on what document should be retrieved.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Can we jointly train the retriever and generator components without any direct supervision on what document should be retrieved\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
